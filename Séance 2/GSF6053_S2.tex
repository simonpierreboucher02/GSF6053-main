\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usetheme{default}
\usecolortheme{default}

\title[S02 Régression et MCO]{Section 02 : Régression et Moindres Carrés Ordinaires\\ (Séance 2)}
\subtitle{GSF-6053: Économétrie Financière}
\author[SP. Boucher]{Simon-Pierre Boucher\inst{1}}
\institute[Université Laval]
{
  \inst{1}%
  Département de finance, assurance et immobilier\\
  Faculté des sciences de l'administration\\
  Université Laval}
\date[Hiver 2022]{18 Janvier 2022}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Références}
\textbf{Obligatoires:}
\begin{itemize}
\item \textbf{Notes de cours:} Section 2 (Professeure: Marie-Hélène Gagnon)
\item \textbf{Woolridge:} chapitres 2 à 7
\end{itemize}
\vspace{0.5cm}
\textbf{Complémentaires:}
\begin{itemize}
\item \textbf{Gujarati et Porter:} chapitres 1 à 9.
\item \textbf{Greene:} chapitres 2, 3, 4, 5, 9, 14, 20, appendices C et D
\end{itemize}
\end{frame}

\begin{frame}{Plan de la séance}
  \tableofcontents
\end{frame}

\section{Modèle de Régression}
\frame{\tableofcontents[current]}

\begin{frame}{Modèle de Régression}
\begin{itemize}
\item Espérance conditionnelle d'une variable indépendante \textbf{(X)} conditionnelle à la valeur connue des régresseurs \textbf{(Y)}

\begin{align*}
E(Y \vert X_i)=f(X_i)
\end{align*}
Où $f(X_i)$ est une fonction des variables explicatives $\textbf{X}$
\vspace{0.5cm}
\item Relation conditionnelle linéaire

\begin{align*}
E(Y \vert X_i)=\beta_1 + \beta_2 X_i
\end{align*}
\item Composate de $Y$:
\begin{itemize}
\item Partie systématique ou déterministe, $E(Y\vert X_i)$
\item Partie aléatoire et non systématique, $\mu_i$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Modèle de Régression}
\begin{itemize}
\item Modèle de Régression Linéaire Simple 
\begin{align*}
Y_i = \beta_1 + \beta_2 X_i + \mu_i 
\end{align*}
\item Prendre espérance des deux cotés 
\begin{align*}
E(Y_i \vert X_i)=E(Y \vert X_i)+E(\mu_i)
\end{align*}
Sachant $E(Y_i \vert X_i)=E(Y \vert X_i)$ alors $E(\mu_i)=0$
\item Les paramètres obtenus à partir d'un échantillon sont des estimateurs (notés avec \hspace{0.1cm} $\hat{}$ \hspace{0.1cm})
\begin{align*}
\hat{Y}_i=\hat{\beta}_1+\hat{\beta}_2 X_i
\end{align*}
\begin{align*}
Y_i = \hat{\beta}_1+\hat{\beta}_2 X_i+\hat{\mu}_i
\end{align*}
$\hat{\mu}_i$ représente l'estimateur du terme d'erreur, le résidu
\end{itemize}
\end{frame}

\section{Moindres Carrés Ordinaires (En sommation)}

\frame{\tableofcontents[current]}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Définissons les résidus
\begin{align*}
\hat{\mu}_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_1-\hat{\beta}_2 X_i
\end{align*}
\item Choisir estimateurs de $\hat{\beta}_1$ et $\hat{\beta_2}$ de facon à minimiser la somme des carrés des résidus 
\begin{align*}
\sum_{i=1}^{N} \hat{\mu}_i^2 & = \left[\hat{\mu}_1^2 +\hat{\mu}_2^2+...+\hat{\mu}_N^2 \right] \\ & = \sum_{i=1}^{N}(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)^2
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Minimiser la somme des carrés des résidus par rapport à $\hat{\beta}_1$ et $\hat{\beta}_2$
\begin{align*}
\min_{\hat{\beta}_1,\hat{\beta}_2} \sum_{i=1}^{N}(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)^2=\min_{\hat{\beta}_1,\hat{\beta}_2} \sum_{i=1}^{N} \hat{\mu}_i^2
\end{align*}
\item Équation normales 
\begin{equation}
\frac{\partial \sum_{i=1}^{N} \hat{\mu}_i^2}{\partial \hat{\beta_1}}=-2\sum_{i=1}^N(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)=0
\end{equation}
\begin{equation}
\frac{\partial \sum_{i=1}^{N} \hat{\mu}_i^2}{\partial \hat{\beta_2}}=-2\sum_{i=1}^NX_i(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)=0
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Solution de $\hat{\beta}_1$ à l'aide de l'équation (1)
\begin{align*}
-2\sum_{i=1}^N(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)=0
\end{align*}
\begin{align*}
\sum_{i=1}^N Y_i - N\hat{\beta}_1-\hat{\beta}_2 \sum_{i=1}^N X_i=0
\end{align*}
\begin{align*}
N\hat{\beta}_1=\sum_{i=1}^N Y_i - \hat{\beta}_2 \sum_{i=1}^N X_i
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Solution de $\hat{\beta}_1$ à l'aide de l'équation (1)

\vspace{0.5cm}
On pose la propriété suivante: 
\begin{itemize}
\item $\sum_{i=1}^N Y_i=N\overline{Y}$ 
\item $\sum_{i=1}^N X_i=N\overline{X}$
\end{itemize}
\begin{align*}
N\hat{\beta}_1 = N\overline{Y} - N \hat{\beta}_2 \overline{X}
\end{align*}
\begin{align*}
\hat{\beta}_1=\frac{N\overline{Y}}{N}-\frac{N\hat{\beta}_2\overline{X}}{N}
\end{align*}

\begin{block}{Solution de $\hat{\beta}_1$}
\begin{align*}
\hat{\beta}_1=\overline{Y}-\hat{\beta}_2 \overline{X}
\end{align*}
\end{block}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Solution de $\hat{\beta}_2$ à l'aide de l'équation (2)
\begin{align*}
-2\sum_{i=1}^NX_i(Y_i-\hat{\beta}_1-\hat{\beta}_2 X_i)=0
\end{align*}
\begin{align*}
\sum_{i=1}^N X_i Y_i - \hat{\beta}_1 \sum_{i=1}^N X_i -\hat{\beta}_2\sum_{i=1}^N X_i^2=0
\end{align*}
Sachant $\overline{Y}-\hat{\beta}_2 \overline{X}$
\begin{align*}
\sum_{i=1}^N X_i Y_i - [\overline{Y}-\hat{\beta}_2 \overline{X}] \sum_{i=1}^N X_i -\hat{\beta}_2\sum_{i=1}^N X_i^2=0
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Solution de $\hat{\beta}_2$ à l'aide de l'équation (2)
\begin{align*}
\sum_{i=1}^N X_i Y_i-\overline{Y}\sum_{i=1}^N X_i \hat{\beta}_2 \overline{X} \sum_{i=1}^N X_i -\hat{\beta}_2 \sum_{i=1}^N X_i^2=0
\end{align*}
\begin{align*}
\sum_{i=1}^N X_i Y_i-N \overline{X} \overline{Y}+\hat{\beta}_2 N \overline{X}^2-\hat{\beta}_2 \sum_{i=1}^N X_i^2=0
\end{align*}
\begin{align*}
\sum_{i=1}^N X_i Y_i-N \overline{X} \overline{Y}=\hat{\beta}_2 \sum_{i=1}^N X_i^2-\hat{\beta}_2 N \overline{X}^2
\end{align*}
\begin{align*}
\sum_{i=1}^N X_i Y_i-N \overline{X} \overline{Y}= \hat{\beta}_2 \left[ \sum_{i=1}^N X_i^2- N \overline{X}^2 \right]
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (En sommation)}
\begin{itemize}
\item Solution de $\hat{\beta}_2$ à l'aide de l'équation (2)
\begin{align*}
\hat{\beta}_2=\frac{\sum_{i=1}^N X_i Y_i-N \overline{X} \overline{Y}}{\sum_{i=1}^N X_i^2- N \overline{X}^2}
\end{align*}
Sachant que 
\begin{itemize}
\item $\sum_{i=1}^N X_i Y_i-N \overline{X} \overline{Y}=\sum_{i=1}^N(X_i-\overline{X})(Y_i-\overline{Y})$
\item $\sum_{i=1}^N X_i^2 - N \overline{X}^2 =\sum_{i=1}^N(X_i-\overline{X})^2$
\end{itemize}
\begin{block}{Solution de $\hat{\beta}_2$}
\begin{align*}
\hat{\beta}_2=\frac{\sum_{i=1}^N(X_i-\overline{X})(Y_i-\overline{Y})}{\sum_{i=1}^N(X_i-\overline{X})^2}
\end{align*}
\end{block}
\end{itemize}
\end{frame}

\section{Moindres Carrés Ordinaires (Format Matricielle)}

\frame{\tableofcontents[current]}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\begin{itemize}

\item Vecteur de la variable à expliquer \textbf{(n $\times$ 1)}
$$Y=\begin{pmatrix}
y_1\\ 
y_2\\ 
\vdots\\
y_n\\
\end{pmatrix}$$
\item Matrice de variables explicatives \textbf{(n $\times$ (k+1))}
$$X=\begin{bmatrix}
 1&x_{11}  &x_{21}  &\cdots   &x_{k1} \\ 
 1&x_{12}  &\cdots  & \cdots &x_{k2} \\ 
 1& \vdots  & \vdots  & \ddots  & \vdots\\ 
 1&x_{1n}  & \cdots & \cdots &x_{kn}
\end{bmatrix}$$
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\begin{itemize}
\item Vecteur de coefficients associés \textbf{(k+1 $\times$ 1)}
$$\beta=\begin{pmatrix}
\beta_0\\ 
\beta_1\\ 
\beta_2\\ 
\vdots\\
\beta_k\\
\end{pmatrix}$$
\item Vecteur associé au terme d'erreur \textbf{(n $\times$ 1)}
$$u=\begin{pmatrix}
u_1\\ 
u_2\\ 
\vdots\\
u_n\\
\end{pmatrix}$$
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\begin{itemize}
\item Modèle Univarié 
\begin{align*}
\begin{pmatrix}
y_1\\ 
y_2\\ 
\vdots\\
y_n\\
\end{pmatrix}=\begin{bmatrix}
 1&x_{11}  &x_{21}  &\cdots   &x_{k1} \\ 
 1&x_{12}  &\cdots  & \cdots &x_{k2} \\ 
 1& \vdots  & \vdots  & \ddots  & \vdots\\ 
 1&x_{1n}  & \cdots & \cdots &x_{kn}
\end{bmatrix} \begin{pmatrix}
\beta_0\\ 
\beta_1\\ 
\beta_2\\ 
\vdots\\
\beta_k\\
\end{pmatrix}+\begin{pmatrix}
u_1\\ 
u_2\\ 
\vdots\\
u_n\\
\end{pmatrix}
\end{align*}
Que nous réécrirons comme suit: 
\begin{align*}
Y=X \beta + u 
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\begin{itemize}
\item Hypothèses distributionnelles sur $u$
\begin{itemize}
\item $u \sim iid$ et $E(u)=0$
\item $V(u)=\sigma^2 I$
\item $u \sim$ iid $N(0,\sigma^2)$
\end{itemize}
\begin{align*}
E(u)=\begin{pmatrix}
E(u_1)=0\\ 
E(u_2)=0\\ 
\vdots\\
E(u_n)=0\\
\end{pmatrix}
\end{align*}
\item Il n'y a pas de corrélation sérielle des erreurs 
\begin{align*}
V(u)=E(uu')=\sigma^2 I 
\end{align*}
\item Découle de l'indépendance
\begin{align*}
COV=[u_i, u_j]=0
\end{align*}
Pour $i \ne j$ $\forall i$ et $\forall j$
\end{itemize}

\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\begin{align*}
E(uu')=E\begin{pmatrix}
u_1\\ 
u_2\\ 
\vdots\\
u_n\\
\end{pmatrix} \begin{pmatrix}
u_1 &u_2 & \cdots &u_n\\ 
\end{pmatrix}
\end{align*}
\begin{align*}
E(uu')=E\begin{pmatrix}
u_1^2 & u_1 u_2 & \cdots &u_1u_n\\ 
u_1u_2 &u_2^2 & \cdots & u_2u_n\\ 
\vdots & \vdots & \ddots & \vdots \\
u_1u_n & u_2u_n & \cdots & u_n^2
\end{pmatrix} 
\end{align*}
\begin{align*}
E(uu')=\begin{pmatrix}
E(u_1^2) & E(u_1 u_2) & \cdots &E(u_1u_n)\\ 
E(u_1u_2) &E(u_2^2) & \cdots & E(u_2u_n)\\ 
\vdots & \vdots & \ddots & \vdots \\
E(u_1u_n) & E(u_2u_n) & \cdots & E(u_n^2)
\end{pmatrix} 
\end{align*}
\end{frame}

\begin{frame}{Moindres Carrés Ordinaires (Format Matricielle)}
\textbf{Rappel:}
\begin{align*}
Var(u)=E(u)^2-[E(u)]^2
\end{align*}
\textbf{Avec Hypothèse d'indépendance:}
\begin{align*}
V(u)= \begin{pmatrix}
\sigma^2 &\cdots &0 \\ 
\vdots &\sigma^2 & \vdots \\ 
0 & \cdots & \sigma^2
\end{pmatrix} = \sigma^2\begin{pmatrix}
1 &\cdots &0 \\ 
\vdots &1 & \vdots \\ 
0 & \cdots & 1
\end{pmatrix} 
\end{align*}
\end{frame}


\begin{frame}{Dérivation MCO (Format Matricielle)}
\begin{align*}
\hat{\beta} = ARGMIN_{\beta} (\hat{u}'\hat{u})
\end{align*}
\begin{align*}
\hat{\beta} = ARGMIN_{\beta} ((Y-X \hat{\beta})'(Y-X\hat{\beta}))
\end{align*}
Distribution des termes de la multiplication matricielle:
\begin{align*}
(Y-X \beta)' (Y-X \beta)=Y'Y-Y'X \hat{\beta}-\hat{\beta}'X'Y +\hat{\beta}'X'X \hat{\beta}
\end{align*}
Sachant 
\begin{align*}
Y'X \hat{\beta}=\hat{\beta}'X'Y
\end{align*}
Alors:
\begin{align*}
(Y-X \beta)' (Y-X \beta)=Y'Y -2Y'X \hat{\beta}+\hat{\beta}'X'X \hat{\beta}
\end{align*}
\end{frame}
\begin{frame}{Dérivation MCO (Format Matricielle)}
\begin{itemize}
\item Dérivé de la somme des résidus au carrés par rapport à $\beta$

\begin{align*}
\frac{\partial (RSS)}{\partial \hat{\beta}}=\frac{\partial (Y'Y -2Y'X \hat{\beta}+\hat{\beta}'X'X \hat{\beta})}{\partial \hat{\beta}}
\end{align*}
Sachant
\begin{align*}
\frac{\partial ([Y'X]\hat{\beta})}{\partial \hat{\beta}}=[Y'X]=X'Y
\end{align*}
\begin{align*}
\frac{\partial (\hat{\beta}'[X'X]\hat{\beta})}{\partial \hat{\beta}}=2[X'X]\hat{\beta}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Dérivation MCO (Format Matricielle)}
\begin{align*}
\frac{\partial (RSS)}{\partial \hat{\beta}}=-2X'Y+2X'X \hat{\beta}=0
\end{align*}
\begin{align*}
X'Y=X'X\hat{\beta}
\end{align*}
\begin{align*}
\hat{\beta}=(X'X)^{-1}X'Y
\end{align*}
\begin{block}{Estimateur de $\hat{\beta}$ par MCO}
\begin{align*}
\hat{\beta}=(X'X)^{-1}X'Y
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Dérivation MCO (Format Matricielle)}
\begin{itemize}
\item Il n'existe pas d'estimateur de $\sigma^2$ pour la méthode des Moindres carrés ordinaires 
\item On peut estimer empiriquement comme suit
\begin{align*}
\hat{\sigma^2}= \frac{1}{T}\sum_{i=1}^n \hat{u}_i=\frac{\hat{u}'\hat{u}}{n}
\end{align*}
\end{itemize}
\end{frame}

\section{Exemple Numérique}

\frame{\tableofcontents[current]}

\begin{frame}{Exemple Numérique}
\begin{itemize}
\item Nous allons générer des variables aléatoires à l’aide d’un modèle dont nous allons déterminer les paramètres.
\item Nous allons utiliser un modèle linéaire simple ayant le format suivant:
\begin{align*}
y=\beta_1+\beta_2 x_i + \epsilon_i
\end{align*}
\begin{itemize}
\item L'intercepte de notre modèle sera de $\beta_1 = 60$.
\item La pente de notre modèle sera de $\beta_2=2$.
\item Nous allons faire l'hypothèse que notre terme d'erreur suit une loi normale de moyenne 0 et variance 4 ($\epsilon \sim N(0,4)$).
\end{itemize}
\item Il nous est maintenant possible de déterminer une valeur pour chaque $y_i$ en utilisant les $x_i$.
\item Pour les $x_i$ nous allons générer uniformément 40 observations entre 0 et 40. 
\end{itemize}
\end{frame}

\begin{frame}{Nuage de points et régression linéaire ($\epsilon \sim N(0,2)$)}

\includegraphics[width=10cm, height=8cm]{ols_2.png}
\end{frame}

\begin{frame}{Nuage de points et régression linéaire ($\epsilon \sim N(0,4)$)}

\includegraphics[width=10cm, height=8cm]{ols_4.png}
\end{frame}

\begin{frame}{Nuage de points et régression linéaire ($\epsilon \sim N(0,6)$)}

\includegraphics[width=10cm, height=8cm]{ols_6.png}
\end{frame}

\begin{frame}{Nuage de points et régression linéaire ($\epsilon \sim N(0,8)$)}

\includegraphics[width=10cm, height=8cm]{ols_8.png}
\end{frame}

\begin{frame}{Exemple Numérique}
\begin{table}
\begin{center}
\begin{tabular}{l c c c c}
\hline
 & $\epsilon \sim N(0,2)$ & $\epsilon \sim N(0,4)$ & $\epsilon \sim N(0,6)$ & $\epsilon \sim N(0,8)$ \\
\hline
(Intercept) & $59.30^{***}$ & $59.28^{***}$ & $60.89^{***}$ & $55.68^{***}$ \\
            & $(0.68)$      & $(1.35)$      & $(2.43)$      & $(2.31)$      \\
x           & $2.02^{***}$  & $2.03^{***}$  & $2.00^{***}$  & $2.13^{***}$  \\
            & $(0.03)$      & $(0.06)$      & $(0.10)$      & $(0.10)$      \\
\hline
R$^2$       & $0.99$        & $0.96$        & $0.89$        & $0.91$        \\
Adj. R$^2$  & $0.99$        & $0.96$        & $0.89$        & $0.91$        \\
Num. obs.   & $50$          & $50$          & $50$          & $50$          \\
\hline
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{Table de régression}
\label{table:coefficients}
\end{center}
\end{table}
\end{frame}

\end{document}




