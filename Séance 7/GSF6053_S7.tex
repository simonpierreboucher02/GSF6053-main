\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usetheme{default}
\usecolortheme{default}

\title[S03 Extensions MLS]{Section 03 : Extensions au mod√®le lin√©aire simple\\ (S√©ance 7)}
\subtitle{GSF-6053: √âconom√©trie Financi√®re}
\author[SP. Boucher]{Simon-Pierre Boucher\inst{1}}
\institute[Universit√© Laval]
{
  \inst{1}%
  D√©partement de finance, assurance et immobilier\\
  Facult√© des sciences de l'administration\\
  Universit√© Laval}
\date[Hiver 2022]{22 f√©vrier 2022}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{R√©f√©rences}
\textbf{Obligatoires:}
\begin{itemize}
\item \textbf{Notes de cours:} Section 3 (Professeure: Marie-H√©l√®ne Gagnon)
\item \textbf{Woolridge:} chapitres 3, 8, 12.
\end{itemize}
\vspace{0.5cm}
\textbf{Compl√©mentaires:}
\begin{itemize}
\item \textbf{Gujarati et Porter:} chapitres 10, 11, 12, 13 et appendice C.
\item \textbf{Greene:} chapitres 2, 3, 4, 5, 9, 14, 20 C et D
\end{itemize}
\end{frame}


\begin{frame}{Plan de la s√©ance}
  \tableofcontents
\end{frame}

\section{Autocorr√©lation des erreurs}

\frame{\tableofcontents[current]}


\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item L‚Äôautocorr√©lation ou la corr√©lation s√©rielle des erreurs dans les s√©ries chronologiques est assez fr√©quente. 
\item Elle d√©coule souvent du fait qu‚Äôil y a une certaine ¬´ inertie ¬ª dans les donn√©es √©conomiques et financi√®res, c‚Äôest-√†-dire que les observations pass√©es se refl√®tent souvent dans les observations pr√©sentes et futures. 
\item Cela peut amener des probl√®mes de corr√©lation s√©rielle des erreurs si une variable autocorrel√©e est omise du mod√®le, par exemple.
\item Il est important de s‚Äôassurer lorsque vous d√©tecter de l‚Äôautocorr√©lation qu‚Äôelle n‚Äôest pas d√ª √† la non-stationnarit√© de Y et X. 
\item En effet, si ces deux quantit√©s sont non stationnaires, les erreurs le seront possiblement aussi et seront possiblement autocorrel√©es.

\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item M√™me si le mod√®le est bien sp√©cifi√©, on peut penser que les erreurs (les chocs) affectant les march√©s boursiers aujourd‚Äôhui ont des chances d‚Äôinfluencer la magnitude et le signe des chocs affectant les march√©s boursiers demain aussi.
\item Dans la plupart des cas en finance, l‚Äôautocorr√©lation sera positive, mais il est possible en th√©orie d‚Äôavoir de l‚Äôautocorr√©lation n√©gative.
\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Soit le mod√®le de base :}
\begin{align*}
y=X \beta +u
\end{align*}
\begin{align*}
Y_t=\beta_0+\beta_1 X_{1t}+\beta_2 X_{2t}+\cdots+\beta_{K-1} X_{K-1t}+u_t
\end{align*}
\begin{align*}
u_t=\rho u_{t-1}+\epsilon_t
\end{align*}
O√π $\epsilon_t$ est un bruit blanc de moyenne nulle et de variance constante dans le temps.
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item La matrice de variance covariance des erreurs sera affect√©e, ce qui impliquera comme dans le cas h√©t√©rosc√©dasticitique que l‚Äôestimateur OLS standard ne sera pas BLUE et que la variance des estimateurs OLS fausse, bien que l‚Äôestimateur sera sans biais. 
\item La d√©monstration de ceci est du m√™me ressort que celle faite pour l‚Äôh√©t√©rosc√©dasticit√©.
\item En effet, bien que l‚Äôautocorr√©lation implique des erreurs homosc√©dastiques (la variance est constante), les termes hors diagonale ne sont pas nuls comme dans le cas de base du mod√®le lin√©aire. 
\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Regardons la matrice de variance-covariance des erreurs:}
\begin{align*}
E(uu')=\begin{bmatrix}
E(u_1^2) & E(u_1u_2) & E(u_1u_3) & \cdots & E(u_1u_T)\\
E(u_2u_1) & E(u_2^2) & E(u_2u_3) & \cdots & E(u_2u_T)\\
E(u_3u_1) & E(u_3u_2) & E(u_3^2) & \cdots & E(u_3u_T)\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
E(u_Tu_1) & E(u_Tu_2) & E(u_Tu_3) & \cdots & E(u_T^2)
\end{bmatrix}
\end{align*}
\begin{align*}
E(uu')=\begin{bmatrix}
\sigma_u^2 & \rho \sigma_u^2 & \rho^2 \sigma_u^2 & \cdots & \rho^{T-1} \sigma_u^2 \\
\rho \sigma_u^2 & \sigma_u^2 & \rho \sigma_u^2 & \cdots & \rho^{T-2} \sigma_u^2 \\
\rho^2 \sigma_u^2 & \rho \sigma_u^2 & \sigma_u^2 & \cdots & \rho^{T-3} \sigma_u^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho^{T-1} \sigma_u^2 & \rho^{T-2} \sigma_u^2 & \rho^{T-3} \sigma_u^2 & \cdots & \sigma_u^2
\end{bmatrix}
\end{align*}
\end{frame}


\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Regardons la matrice de variance-covariance des erreurs:}
\begin{align*}
E(uu')=\sigma_u^2\begin{bmatrix}
1 & \sigma & \sigma^2 & \cdots & \rho^{T-1}\\
\rho & 1 & \rho & \cdots & \rho^{T-2} \\
\rho^2 & \rho & 1 & \cdots & \rho^{T-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
\rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1
\end{bmatrix}
\end{align*}
\begin{align*}
E(uu')= \frac{\sigma_{\epsilon}^2}{1-\rho^2} \begin{bmatrix}
1 & \sigma & \sigma^2 & \cdots & \rho^{T-1}\\
\rho & 1 & \rho & \cdots & \rho^{T-2} \\
\rho^2 & \rho & 1 & \cdots & \rho^{T-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
\rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1
\end{bmatrix}=\sigma_{\epsilon}^2 \Omega
\end{align*}
\end{frame}



\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Composition de $\Omega$:}
\begin{align*}
\Omega = \frac{1}{1-\rho^2}\begin{bmatrix}
1 & \sigma & \sigma^2 & \cdots & \rho^{T-1}\\
\rho & 1 & \rho & \cdots & \rho^{T-2} \\
\rho^2 & \rho & 1 & \cdots & \rho^{T-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
\rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1
\end{bmatrix}
\end{align*}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Hypoth√®ses de d√©part:}
\begin{block}{$1-\epsilon_t$ est un bruit blanc}
\begin{itemize}
\item $E(\epsilon_t)=0$
\item $V(\epsilon_t)=\sigma_{\epsilon}^2 \forall t$
\item $E(\epsilon_t\epsilon_s)=0 \forall t \neq s$
\end{itemize}
\end{block}

\begin{block}{$2-\mid \rho \mid < 1$}
\begin{itemize}
\item 1 sera une valeur critique. (Racine unitaire)
\item Ceci implique que $u_t$ est un processus stationnaire
\begin{enumerate}
\item $E(u_t)=0$
\item $V(u)=\sigma_u^2, \forall t$
\item $E(u_tu_{t-j})=E(u_su_{s-j}) \forall t,s$
\end{enumerate}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\textbf{Hypoth√®ses de d√©part:}
\begin{block}{$3-\epsilon_t$ n'est pas corr√©l√© avec les $u_s$ donc l'indice (s) est ant√©rieur √† t. $\rightarrow \epsilon_t$ n'est pas corr√©l√© avec $u_{t-1}$}
√âcrivons la s√©rie des $u_t$ en fonction des chocs ùúÄùë° pr√©sents et pass√©s:
\begin{align*}
u_t & =\rho u_{t-1}+\epsilon_t \\ & = \rho [\rho u_{t-2}+\epsilon_{t-1}]+\epsilon_t \\ & = \rho^2 u_{t-2}+\rho \epsilon_{t-1}+\epsilon_t \\ & = \rho^2[\rho u_{t-3}+\epsilon_{t-2}]+\rho \epsilon_{t-1}+\epsilon_t \\ & = \rho^3 u_{t-3}+\rho^2 \epsilon_{t-2}+\rho \epsilon_{t-1}+\epsilon_t \\ & = \rho^su_{t-s}+\epsilon_t+\rho \epsilon_{t-1}+\rho^2 \epsilon_{t-2}+\cdots + \rho^{s-1}\epsilon_{t-(s-1)}
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
Consid√©rons les possibilit√©s suivantes :
\begin{itemize}
\item $u_0=0$ $\rightarrow$ la valeur initiale du processus des $u_t=0$ 
\begin{align*}
(\rightarrow u_{t-s}=0 \hspace{0.2cm} \textbf{si} \hspace{0.2cm} t-s=0)
\end{align*}
\item Le processus des $u_t$ a commenc√© au pass√© infini.
\begin{align*}
(\lim_{s \rightarrow \infty} \rho^s=0)
\end{align*}
\item Dans les deux cas, si je recule assez dans le temps, la contribution de la valeur initiale est :
\begin{align*}
\rho^s u_{t-s}=0
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item Les $u_t$ peuvent donc √™tre r√©√©crit comme une somme de bruits blancs pond√©r√©s :
\begin{align*}
u_t = \epsilon_t+\rho \epsilon_{t-1} + \rho^2 \epsilon_{t-2} + \rho^3 \epsilon_{t-3}+\cdots+\rho^{s-1}\epsilon_{t-(s-1)}
\end{align*}
\item On voit que l‚Äôeffet des chocs les plus proches dans le pass√© est le plus important.
\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item Calculons la variance de $u_t$
\begin{align*}
\sigma_u^2 & =V(\epsilon_t)+\rho^2V(\epsilon_{t-1})+\rho^4V(\epsilon_{t-2})+...+\rho^{2(s-1)}V(\epsilon_{t-(s-1)}) \\ & = \sigma_{\epsilon}^2+\rho^2\sigma_{\epsilon}^2+\rho^4\sigma_{\epsilon}^2+\rho^6\sigma_{\epsilon}^2+\rho^{2(s-1)}\sigma_{\epsilon}^2 \\ & = \sigma_{\epsilon}^2(1+\rho^2+\rho^4+\rho^6+\rho^{2(s-1)}) \\ & = \sigma_{\epsilon}^2 \left( \frac{1}{1-\rho^2} \right)
\end{align*}
La stationnarit√© implique:
\begin{align*}
E(u_{t}^2)=E(u_{t-1}^2)
\end{align*}
Ainsi $\sigma_{u}^2=\rho^2 \sigma_{u}^2+\sigma_{\epsilon}^2$ et $\sigma_{u}^2=\left(\frac{\sigma_{\epsilon}^2}{1-\rho^2} \right)$
\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item On a donc d√©montr√© d‚Äôo√π viennent les coefficients sur la diagonale de la matrice $\Omega$.
\begin{block}{Calculez $E(u_tu_{t-1})$}
\begin{align*}
u_t = \epsilon_t +\rho \epsilon_{t-1}+\rho^{2}\epsilon_{t-2}+\rho^{3} \epsilon_{t-3}+\cdots
\end{align*}
\begin{align*}
u_{t-1} = \epsilon_{t-1} +\rho \epsilon_{t-2}+\rho^{2}\epsilon_{t-3}+\rho^{3} \epsilon_{t-4}+\cdots
\end{align*}
\end{block}
\end{itemize}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{block}{Rappel: $E(\epsilon_t \epsilon_s)=0 \forall t \neq s$}
\begin{align*}
E(u_t u_{t-1}) & = \rho E(\epsilon_{t-1}^2)+\rho^3 E(\epsilon_{t-2}^2)+\rho^5E(\epsilon_{t-3}^2)+...\\ & = \rho\sigma_{\epsilon}^2+\rho^3 \sigma_{\epsilon}^2+\rho^5 \sigma_{\epsilon}^2+... \\ & = \rho \sigma_{\epsilon}^2(1+\rho^2+\rho^4+...) \\ & = \rho \sigma_{\epsilon}^2 \frac{1}{1-\rho^2} \\ & = \rho \sigma_u^2
\end{align*}
\end{block}

\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{block}{Rappel: $E(\epsilon_t \epsilon_s)=0 \forall t \neq s$}
Par le m√™me raisonnement, on peut d√©montrer que
\begin{align*}
E(u_tu_{t-s})=\rho^s\sigma_{\epsilon}^2\frac{1}{1-\rho^2}=\rho^s\sigma_u^2
\end{align*}
\end{block}

\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
Preuve alternative qui ne passe pas par l‚Äôinfini, mais par l‚Äôhypoth√®se que $\epsilon_t$ n‚Äôest pas corr√©l√© avec les $u_t$ ant√©rieurs.
\begin{align*}
u_t=\rho^s u_{t-s} +\epsilon_{t}+\rho \epsilon_{t-1} \rho^2 \epsilon_{t-2}+ \cdots+ \rho^{s-1} \epsilon_{t-(s-1)}
\end{align*}
\begin{align*}
u_tu_{t-s}=\rho^s u_{t-s}^2 +\epsilon_{t}u_{t-s}+\rho \epsilon_{t-1}u_{t-s} \rho^2 \epsilon_{t-2}u_{t-s} \\ + \cdots+ \rho^{s-1} \epsilon_{t-(s-1)}u_{t-s}
\end{align*}
\begin{align*}
E(u_tu_{t-s})=\rho^s E(u_{t-s}^2) +E(\epsilon_{t}u_{t-s})+\rho E(\epsilon_{t-1}u_{t-s}) \rho^2 E(\epsilon_{t-2}u_{t-s}) \\ + \cdots+ \rho^{s-1} E(\epsilon_{t-(s-1)}u_{t-s})
\end{align*}

\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{align*}
E(u_tu_{t-s})=\rho^s E(u_{t-s}^2)
\end{align*}
Sachant que 
\begin{align*}
E(u_{t-s}^2)=\sigma_u^2= \sigma_{\epsilon}^2 \frac{1}{1-\rho^2}
\end{align*}
\begin{align*}
E(u_tu_{t-s})= \rho^s \left( \frac{\sigma_{\epsilon}^2}{1-\rho^2} \right)= \rho^s \sigma_u^2
\end{align*}
\end{frame}

\begin{frame}{Autocorr√©lation des erreurs}
\begin{itemize}
\item Nous venons donc de d√©montrer comment la matrice de variance-covariance est obtenue selon diff√©rentes hypoth√®ses de d√©part. 
\item On peut d√©montrer que la matrice $P^{-1}$ associ√©e sera :
\end{itemize}
\begin{align*}
P^{-1}=\begin{bmatrix}
\sqrt{1-\rho^2} & 0 & 0 & \cdots & 0 & 0 \\
-\rho & 1 & 0 \cdots & 0 & 0 \\
0 & -\rho & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & 0 \\
0 & 0 & 0 & \cdots & -\rho & 1 
\end{bmatrix}
\end{align*}
\end{frame}

\section{Transformation de Prais-Waisten}

\frame{\tableofcontents[current]}


\begin{frame}{Transformation de Prais-Waisten}
\begin{itemize}
\item La transformation de Prais-Waisten consiste √† multiplier  les √©l√©ments de la r√©gression par la matrice $P^{-1}$.
\begin{align*}

P^{-1} Y = \begin{bmatrix}
\sqrt{1-\rho^2} & 0 & 0 & \cdots & 0 & 0 \\
-\rho & 1 & 0 \cdots & 0 & 0 \\
0 & -\rho & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & 0 \\
0 & 0 & 0 & \cdots & -\rho & 1 
\end{bmatrix}  \begin{bmatrix}
Y_1 \\
Y_2 \\
Y_3 \\
\vdots \\
Y_{T-1}\\
Y_T
\end{bmatrix} 

\end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Transformation de Prais-Waisten}
\begin{itemize}
\item La transformation de Prais-Waisten consiste √† multiplier  les √©l√©ments de la r√©gression par la matrice $P^{-1}$
\end{itemize}

\begin{align*}
P^{-1} Y = \begin{bmatrix}
\sqrt{1-\rho^2}Y_1 \\
-\rho Y_1 + Y_2 \\
-\rho Y_2 + Y_2 \\
\vdots \\
-\rho Y_{T-2}+Y_{T-1}\\
=\rho Y_{T-1} + Y_T 
\end{bmatrix} 
\end{align*}

\end{frame}

\begin{frame}{Transformation de Prais-Waisten}
\begin{itemize}
\item La transformation sera la m√™me pour chaque r√©gresseur X. 
\item Le mod√®le transform√© s‚Äô√©crit :
\begin{align*}
\sqrt{1-\rho^2}Y_1=\sqrt{1-\rho^2}\beta_0 + \beta_1 \sqrt{1-\rho^2}X_{11}+\beta_2 \sqrt{1-\rho^2}X_{21}\\ + \cdots+ \sqrt{1-\rho^2}u_1
\end{align*}
\begin{align*}
Y_t-\rho Y_{t-1}=(1-\rho) \beta_0+\beta_1 (X_{1t}-\rho X_{1,T-1})+\beta_2 (X_ {2t}-\rho X_{2,t-1}) \\ + \cdots +\beta_{k-1} (X_{kt}-\rho X_{k-1,t-1})+u_t-\rho u_{t-1}
\end{align*}
\item Il faut porter attention √† la constante dans ce mod√®le qui n‚Äôest plus la constante d‚Äôorigine.
\item Si on a $\rho$ inconnu, ce mod√®le n‚Äôest plus lin√©aire, mais on peut toujours l‚Äôestimer pas OLS.
\end{itemize}
\end{frame}


\section{Transformation de Cochran-Orcutt}

\frame{\tableofcontents[current]}


\begin{frame}{Transformation de Cochran-Orcutt}
\begin{itemize}
\item Une autre fa√ßon de transformer le mod√®le serait le laisser tomber la premi√®re observation et de transformer les Y comme suit :
\begin{align*}
Y_t=\rho Y_{t-1}, t=2,...,T
\end{align*}
\item Les X sont transform√©s de la m√™me facon:
\begin{align*}
X_{it}-\rho X_{i,t-1}, t=2,...,T
\end{align*}
\item Le mod√®le transform√© de Cochran-Orcutt s‚Äô√©crit de la m√™me fa√ßon que Prais-Winsten, mais sans la premi√®re observation :
\begin{align*}
Y_t-\rho Y_{t-1}=(1-\rho) \beta_0+\beta_1 (X_{1t}-\rho X_{1,T-1})+\beta_2 (X_ {2t}-\rho X_{2,t-1}) \\ + \cdots +\beta_{k-1} (X_{kt}-\rho X_{k-1,t-1})+u_t-\rho u_{t-1}
\end{align*} 
\end{itemize}
\end{frame}

\section{Le probl√®me de $\rho$}

\frame{\tableofcontents[current]}


\begin{frame}{Le probl√®me de $\rho$}
\begin{itemize}
\item $\rho$ est inconnu.
\item Il faut donc trouver un estimateur.
\item Plusieurs sont sugg√©r√©s dans la litt√©rature √©conom√©trique.
\end{itemize}
\begin{block}{Un estimateur usuel:}
\begin{align*}
\hat{\rho}=\frac{\sum_{t=2}^T \hat{u}_t \hat{u}_{t-1}}{\sum_{t=2}^T \hat{u}_{t-1}^2}
\end{align*}
O√π $\hat{u}_t$ est le r√©sidu OLS de la r√©gression de Y sur X. On peut montrer que $PLIM$ $\hat{\rho}=\rho$
\end{block}
\end{frame}

\begin{frame}{Le probl√®me de $\rho$}
\begin{block}{Estimateur de Hildreth-LU:}
\begin{itemize}
\item Cet estimateur ne passe pas par les OLS.
\item En balayant l‚Äôintervalle $]‚àí1, 1 [$, on peut choisir la valeur de $\rho$ qui minimise la somme des carr√©s des erreurs du mod√®le transform√©. 

\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Le probl√®me de $\rho$}
\begin{block}{Estimateur de Hildreth-LU:}
Les √©tapes sont :
\begin{enumerate}
\item Choisir $\rho^{(1)} = ‚àí1$ (ou -0.99999)
\item Obtenir le mod√®le transform√© correspondant.
\item Appliquer les OLS et sauvegarder $\hat{u}'\hat{u}_{\rho(1)}$.
\item  Choisir une nouvelle valeur : $\rho^{(2)} = \rho^{(1)} + \textbf{step}$
\item Obtenir le mod√®le transform√© correspondant
\item Appliquer les OLS et sauvegarder $\hat{u}'\hat{u}_{\rho (2)}$
\item Continuer jusqu‚Äô√† ce que $]‚àí1, 1 [$ est couvert.
\item Choisir la valeur $\rho^{(s)}$ telle que
\begin{align*}
\hat{u}'\hat{u}_{\rho (s)}=\min_i [\hat{u}'\hat{u}_{\rho (i)}]
\end{align*}
\end{enumerate}
\end{block}
\end{frame}


\begin{frame}{Le probl√®me de $\rho$}
\begin{block}{Estimateur de Hildreth-LU:}
\begin{itemize}
\item L‚Äôestimateur FGLS obtenu √† partir d‚Äôun estimateur de $\rho$ sera convergent. 
\item Donc, lorsque la matrice d‚Äôinformation P d√©pend de param√®tres inconnus qu‚Äôil faut estimer, on perd la propri√©t√© BLUE.
\item L‚Äôestimateur qui est un FGLS au lieu de GLS, reste convergent si l‚Äôestimateur des param√®tres inconnus sur lequel il est fond√© est convergent.
\end{itemize}
\end{block}
\end{frame}

\section{Diagnostique de l'autocorr√©lation}

\frame{\tableofcontents[current]}


\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\begin{itemize}
\item C‚Äôest un test √† borne. 
\item Vous avez deux points critiques √† regarder dans la table.
\item √Ä l‚Äôorigine, ce test √©tait √† borne, car on ne connaissait pas les vrais points critiques seulement des bornes √† ceux-ci sous l‚Äôhypoth√®se de normalit√©.
\item Maintenant, on a trouv√© les vrais points critiques.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\textbf{Hypoth√®ses:}
\begin{itemize}
\item $H_0: \rho=0$ contre l'alternative
\item $H_A: \rho \neq 0$
\end{itemize}
\textbf{Statistique de test:}
\begin{align*}
d=\frac{\sum_{t=2}^T(\hat{u}_t-\hat{u}_{t-1})^2}{\sum_{t=1}^T\hat{u}_{t-1}^2}
\end{align*}
\begin{align*}
d \approx 2(1-\hat{\rho})
\end{align*}
\end{block}
\end{frame}


\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\textbf{Hypoth√®ses du test de Durbin-Watson:}
\begin{enumerate}
\item Le mod√®le de r√©gression inclue une constante pour pouvoir calculer RSS.
\item Les variables explicatives sont non stochastiques.
\item L‚Äôautocorr√©lation est d‚Äôordre 1. On ne peut pas utilise Durbin-Watson avec les ordres sup√©rieurs.
\item On ne peut pas utiliser ce test dans le cas de mod√®les autor√©gressifs.
\item Le test n‚Äôaccommode pas les observations manquantes.
\end{enumerate}
\end{block}
\end{frame}



\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\begin{itemize}
\item Pour faire le test de Durbin-Watson, il s‚Äôagit de rouler la r√©gression OLS et d‚Äôobtenir les r√©sidus. 
\item Ensuite on calcule la statistique de Durbin-Watson avec ces r√©sidus.
\item On compare la statistique avec les points critiques de la distribution pour les statistiques de Durbin-Watson pour une taille d‚Äô√©chantillon donn√© et un K donn√©.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\textbf{R√®gle de d√©cision:}
\begin{itemize}
\item La d√©cision d√©pend de la valeur de d par rapport √† deux points critiques $d_L$ et $d_U$
\begin{itemize}
\item $d < d_L \rightarrow$ rejet de $H_0$
\item $d_L < d < d_U \rightarrow$ test non conclusif 
\item $d_U < d < 4-d_U \rightarrow$ non rejet de $H_0$ et $H_0^*$
\item $4-d_U < d < 4-d_L \rightarrow$ test non conclusif
\item $d > 4-d_L \rightarrow$ rejet de $H_0^*$
\end{itemize}
\item $H_0:$ pas d'autocor√©lation positive
\item $H_0^*:$ pas d'autocor√©lation n√©gative
\item La statistique de Durbin-Watson est une \textbf{institution} en √©conom√©trie parce qu‚Äôil s‚Äôagit d‚Äôun des premiers tests √† borne d√©riv√©s, mais les hypoth√®ses sous-jacentes sont assez contraignantes.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Diagnostique de l'autocorr√©lation}
\begin{block}{Test de Durbin-Watson}
\includegraphics[width=10cm, height=8cm]{DW.png}
\end{block}
\end{frame}

\section{Estimateur Robuste de Newey West}

\frame{\tableofcontents[current]}


\begin{frame}{Estimateur Robuste de Newey West}
\begin{itemize}
\item Il existe une forme de correcteur robuste pour les probl√®mes d‚Äôautocorr√©lation des erreurs qui est une alternative au FGLS quand le nombre d‚Äôobservations est assez √©lev√©.
\item Cette correction est asymptotique.
\item C‚Äôest l‚Äôestimateur de la variance de Newey-West. 
\item L‚Äôintuition est la m√™me que le correcteur de White, mais appliqu√© aux probl√®mes d‚Äôautocorr√©lation des erreurs.
\item En fait, le charme de l‚Äôestimateur de Newey-West c‚Äôest qu‚Äôil corrige pour \textbf{l‚Äôautocorr√©lation} ET \textbf{l‚Äôh√©t√©rosc√©dasticit√© des erreurs} (c‚Äôest ce qu‚Äôon appelle l‚Äôestimateur \textbf{HAC}).
\item Le correcteur de Newey West est d√©j√† programm√© dans les logiciels d‚Äôanalyses de donn√©es comme Stata.
\end{itemize}
\end{frame}

\end{document}